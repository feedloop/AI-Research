{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Synthetic Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from langchain.chat_models import AzureChatOpenAI\n",
    "from langchain.embeddings import AzureOpenAIEmbeddings\n",
    "\n",
    "gpt_35_16k = AzureChatOpenAI(\n",
    "    azure_deployment=os.getenv(\"AZURE_DEPLOYMENT_NAME\"),\n",
    "    model=\"gpt-3.5-turbo-16k\",\n",
    "    azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    api_version=os.getenv(\"AZURE_OPENAI_API_VERSION\"),\n",
    ")\n",
    "\n",
    "gpt_4 = AzureChatOpenAI(\n",
    "    azure_deployment=\"chat-gpt-4\",\n",
    "    model=\"gpt-4\",\n",
    "    azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    api_version=os.getenv(\"AZURE_OPENAI_API_VERSION\"),\n",
    ")\n",
    "\n",
    "embedding_model = AzureOpenAIEmbeddings(\n",
    "    model= \"text-embedding-ada-002\",\n",
    "    azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    api_version=os.getenv(\"AZURE_OPENAI_API_VERSION\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "def check_similarity(row, dataframe):\n",
    "    similarity_threshold = 0.9 \n",
    "    for _, existing_row in dataframe.iterrows():\n",
    "        tfidf_matrix = vectorizer.fit_transform([row['question'], existing_row['question']])\n",
    "        cos_similarity = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])\n",
    "        cos_similarity = cos_similarity[0][0]\n",
    "        if  cos_similarity > similarity_threshold:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def check_dataframe(final_df, df):\n",
    "    rows_to_add = []\n",
    "    for _, row in df.iterrows():\n",
    "        if not check_similarity(row, final_df):\n",
    "            rows_to_add.append(row)\n",
    "    \n",
    "    if rows_to_add:\n",
    "        filtered_new_df = pd.DataFrame(rows_to_add)\n",
    "        final_df = pd.concat([final_df, filtered_new_df], ignore_index=True)\n",
    "    \n",
    "    return final_df\n",
    "\n",
    "\n",
    "def generate_floats():\n",
    "    result = []\n",
    "\n",
    "    first_float = round(random.uniform(0.0, 1.0), 1)\n",
    "    result.append(first_float)\n",
    "    \n",
    "    remaining_sum = 1.0 - first_float\n",
    "    \n",
    "    # Generate subsequent floats based on the remaining sum\n",
    "    for _ in range(3):\n",
    "        if remaining_sum >= 0.09999999999999998:\n",
    "            next_float = round(random.uniform(0.0, remaining_sum), 1)\n",
    "            result.append(next_float)\n",
    "            remaining_sum -= next_float\n",
    "        else:\n",
    "            result.append(0.0)\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "def generate_distribution():\n",
    "    result = generate_floats()\n",
    "    while sum(result) <= 1.0:\n",
    "        result = generate_floats()\n",
    "\n",
    "    testset_distribution = {\n",
    "        \"simple\": 0,\n",
    "        \"multi_context\": 0,\n",
    "        \"reasoning\": 0,\n",
    "        \"conditional\": 0,\n",
    "    }\n",
    "\n",
    "    for dist in testset_distribution.keys():\n",
    "        random_element = random.choice(result)\n",
    "        testset_distribution[dist] = random_element\n",
    "        result.remove(random_element)\n",
    "\n",
    "    return testset_distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import SimpleDirectoryReader\n",
    "\n",
    "reader = SimpleDirectoryReader(input_dir=os.getenv(\"PDF_DIR\"))\n",
    "documents = reader.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.llms import LangchainLLM\n",
    "from ragas.testset import TestsetGenerator\n",
    "\n",
    "final_df = pd.DataFrame([], columns=[\n",
    "    'question',\n",
    "    'ground_truth_context',\n",
    "    'ground_truth',\n",
    "    'question_type',\n",
    "    'episode_done'])\n",
    "\n",
    "iteration = 0\n",
    "\n",
    "while len(final_df) < 100:\n",
    "    testset_distribution = generate_distribution()\n",
    "    print(testset_distribution)\n",
    "    # testset_distribution = {\n",
    "    #     \"simple\": 0.6,\n",
    "    #     \"multi_context\": 0.2,\n",
    "    #     \"reasoning\": 0.1,\n",
    "    #     \"conditional\": 0.1,\n",
    "    # }\n",
    "    \n",
    "    # if iteration % 2 == 0:\n",
    "    #     generator_llm = LangchainLLM(llm=gpt_4)\n",
    "    # else:\n",
    "    generator_llm = LangchainLLM(llm=gpt_35_16k)\n",
    "    critic_llm = LangchainLLM(llm=gpt_4)\n",
    "\n",
    "    test_generator = TestsetGenerator(\n",
    "        generator_llm=generator_llm,\n",
    "        critic_llm=critic_llm,\n",
    "        embeddings_model=embedding_model,\n",
    "        testset_distribution=testset_distribution,\n",
    "        chat_qa=0.0,\n",
    "        chunk_size=512,\n",
    "        threshold=5.0,\n",
    "    )\n",
    "    test_set = test_generator.generate(documents, test_size=100)\n",
    "    test_df = test_set.to_pandas()\n",
    "    final_df = check_dataframe(final_df, test_df)\n",
    "\n",
    "    iteration += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rowIndex(row):\n",
    "    return row.name\n",
    "\n",
    "def generate_expected(row):\n",
    "    truth = ' or '.join(row['ground_truth'])\n",
    "    context = \"\\n\\n\\n\\n\".join(row['ground_truth_context'])\n",
    "    return f\"\"\"The answer must have the same meaning with this answer not less and not more: {truth}\n",
    "Also, the answer must be relevant with these contexts: {context}\"\"\"\n",
    "\n",
    "def generate_scenario_name(row):\n",
    "    return f\"{os.getenv('FILE_NAME')} - {row['rowIndex']}\"\n",
    "\n",
    "# Apply the function to create the 'expected' column\n",
    "final_df[\"rowIndex\"] = final_df.apply(rowIndex, axis=1)\n",
    "final_df[\"expected\"] = final_df.apply(generate_expected, axis=1)\n",
    "final_df[\"scenario name\"] = final_df.apply(generate_scenario_name, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.to_csv(os.getenv(\"OUTPUT PATH\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
