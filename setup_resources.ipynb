{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Imports, Global Var, and Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import psycopg2\n",
    "\n",
    "from tqdm import tqdm\n",
    "from pypdf import PdfReader\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from pgvector.psycopg2 import register_vector\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import db, api\n",
    "\n",
    "conn = psycopg2.connect(os.getenv(\"POSTGRES_CONFIG\"))\n",
    "register_vector(conn)\n",
    "cursor = conn.cursor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Resources Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import Document\n",
    "from llama_index.storage.docstore import SimpleDocumentStore\n",
    "from llama_index.node_parser import (\n",
    "    HierarchicalNodeParser,\n",
    "    SentenceWindowNodeParser,\n",
    "    get_leaf_nodes,\n",
    ")\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "\n",
    "char_tiktoken_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=512, chunk_overlap=100, add_start_index=True,\n",
    ")\n",
    "\n",
    "hierarchical_parser = HierarchicalNodeParser.from_defaults(chunk_sizes=[512, 128], chunk_overlap=20)\n",
    "sent_parser = SentenceWindowNodeParser.from_defaults(\n",
    "    window_size=3,\n",
    "    window_metadata_key=\"window\",\n",
    "    original_text_metadata_key=\"original_text\",\n",
    ")\n",
    "\n",
    "def get_page_summary(page_content, lang=\"Indonesian\"):\n",
    "    answer = api.get_completions_gpt4([\n",
    "        {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": f\"\"\"Give me a comprehensive and complete summary of the provided page content below in {lang}. The summary must include all informations provided in the page content without any exceptions. The summary must be provided in {lang} and less than 100 words, and only provide your summary as response.\n",
    "Page Content: {page_content}\n",
    "Summary:\"\"\",\n",
    "        }\n",
    "    ], temp=0)\n",
    "    summary = answer.choices[0].message.content\n",
    "    if lang == \"Indonesian\":\n",
    "        if \"Ringkasan Konten: \" in summary:\n",
    "            summary = summary.replace(\"Ringkasan Konten: \", \"\")\n",
    "        if \"Ringkasan: \" in summary:\n",
    "            summary = summary.replace(\"Ringkasan: \", \"\")\n",
    "    elif lang == \"English\":\n",
    "        if \"Summary: \" in summary:\n",
    "            summary = summary.replace(\"Summary: \", \"\")\n",
    "    \n",
    "    return summary\n",
    "\n",
    "\n",
    "def setup_resource_nonllm(resource_id, page_num, summary, page_content):\n",
    "    chunks = [chunk.__dict__ for chunk in char_tiktoken_splitter.create_documents([page_content])]\n",
    "    for chunk in chunks:\n",
    "        chunk_content = chunk[\"page_content\"]\n",
    "        embeddings = api.get_embeddings_ada(chunk_content)\n",
    "        data = {\n",
    "            \"context\": summary,\n",
    "            \"fact\": chunk_content,\n",
    "            \"resource_id\": resource_id,\n",
    "            \"embeddings\": embeddings,\n",
    "            \"summary\": summary,\n",
    "            \"number\": page_num + 1\n",
    "        }\n",
    "        db.insert_fact_resource(conn, cursor, data)\n",
    "\n",
    "\n",
    "def setup_resource_summary(resource_id, page_num, summary):\n",
    "    embeddings = api.get_embeddings_ada(summary)\n",
    "    data = {\n",
    "        \"context\": \"\",\n",
    "        \"fact\": summary,\n",
    "        \"resource_id\": resource_id,\n",
    "        \"embeddings\": embeddings,\n",
    "        \"summary\": summary,\n",
    "        \"number\": page_num + 1\n",
    "    }\n",
    "    db.insert_fact_resource(conn, cursor, data)\n",
    "\n",
    "\n",
    "def setup_resource_parent_child(resource_id, page_num, summary, page_content):\n",
    "    docs = [Document(text=page_content)]\n",
    "    nodes = hierarchical_parser.get_nodes_from_documents(docs)\n",
    "    docstore = SimpleDocumentStore()\n",
    "    docstore.add_documents(nodes)\n",
    "\n",
    "    leaves = get_leaf_nodes(nodes)\n",
    "    for leaf in leaves:\n",
    "        parent = docstore.get_document(leaf.parent_node.node_id).text\n",
    "        embeddings = api.get_embeddings_ada(leaf.text)\n",
    "        data = {\n",
    "            \"context\": parent,\n",
    "            \"fact\": leaf.text,\n",
    "            \"resource_id\": resource_id,\n",
    "            \"embeddings\": embeddings,\n",
    "            \"summary\": summary,\n",
    "            \"number\": page_num + 1\n",
    "        }\n",
    "        db.insert_fact_resource(conn, cursor, data)\n",
    "\n",
    "\n",
    "def setup_resource_sent_window(resource_id, page_num, summary, page_content):\n",
    "    docs = [Document(text=page_content)]\n",
    "    nodes = sent_parser.get_nodes_from_documents(docs)\n",
    "\n",
    "    for node in nodes:\n",
    "        embeddings = api.get_embeddings_ada(node.text)\n",
    "        data = {\n",
    "            \"context\": node.metadata[\"window\"],\n",
    "            \"fact\": node.metadata[\"window\"],\n",
    "            \"resource_id\": resource_id,\n",
    "            \"embeddings\": embeddings,\n",
    "            \"summary\": summary,\n",
    "            \"number\": page_num + 1\n",
    "        }\n",
    "        db.insert_fact_resource(conn, cursor, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Resources Setup for NonLLM, Summary, ParentChild, SentWindow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pdf in tqdm(glob.glob(f\"{os.getenv('PDF_DIR')}/*.pdf\")):\n",
    "    \n",
    "    fname = pdf.split(\"\\\\\")[-1].split(\".pdf\")[0]\n",
    "    print(f\"Currently setup for file: {fname}\")\n",
    "    \n",
    "    nonllm_id = db.get_resource_id(cursor, f\"{fname} - NonLLM\")\n",
    "    summary_id = db.get_resource_id(cursor, f\"{fname} - Summary\")\n",
    "    parent_child_id = db.get_resource_id(cursor, f\"{fname} - ParentChild\")\n",
    "    sent_window_id = db.get_resource_id(cursor, f\"{fname} - SentWindow\")\n",
    "    db.delete_facts_resource(conn, cursor, nonllm_id)\n",
    "    db.delete_facts_resource(conn, cursor, summary_id)\n",
    "    db.delete_facts_resource(conn, cursor, parent_child_id)\n",
    "    db.delete_facts_resource(conn, cursor, sent_window_id)\n",
    "\n",
    "    reader = PdfReader(pdf)\n",
    "    for page_num, page in enumerate(reader.pages):\n",
    "        page_content = page.extract_text()\n",
    "        summary = get_page_summary(page_content)\n",
    "\n",
    "        setup_resource_nonllm(nonllm_id, page_num, summary, page_content)\n",
    "        setup_resource_summary(summary_id, page_num, summary)\n",
    "        setup_resource_parent_child(parent_child_id, page_num, summary, page_content)\n",
    "        setup_resource_sent_window(sent_window_id, page_num, summary, page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
