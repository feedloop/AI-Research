{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import psycopg2\n",
    "import pandas as pd\n",
    "import json, ast\n",
    "\n",
    "from pypdf import PdfReader\n",
    "\n",
    "from importlib import reload\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from pgvector.psycopg2 import register_vector\n",
    "\n",
    "from utils import api, db, prompt, prompting\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "conn = psycopg2.connect(os.getenv(\"POSTGRES_CONFIG\"))\n",
    "register_vector(conn)\n",
    "cursor = conn.cursor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Fact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_summary_fact(p_content, user_language):\n",
    "    messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": prompting.summary_prompt(p_content, user_language)\n",
    "    },\n",
    "    ]\n",
    "    completion = api.get_completions_gpt35(messages, 0)\n",
    "    res_json = completion.choices[0].message.content\n",
    "\n",
    "    try:\n",
    "        res_json = ast.literal_eval(res_json)\n",
    "    except:\n",
    "        res_json = ast.literal_eval(res_json.replace(\"null\", \"None\"))\n",
    "\n",
    "    fact_list = [item['chunk'] for item in res_json['chunks']]\n",
    "    current_summary = res_json['summary']\n",
    "    return fact_list, current_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_summary_fact_rec(page, user_language):\n",
    "    messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": prompting.summary_prompt_rec(page, user_language)\n",
    "    },\n",
    "    ]\n",
    "    completion = api.get_completions_gpt35(messages, 0)\n",
    "    \n",
    "    res_json = completion.choices[0].message.content\n",
    "\n",
    "    try:\n",
    "        res_json = ast.literal_eval(res_json)\n",
    "    except:\n",
    "        res_json = ast.literal_eval(res_json.replace(\"null\", \"None\"))\n",
    "\n",
    "    fact_list = [item['chunk'] for item in res_json['chunks']]\n",
    "    current_summary = res_json['summary']\n",
    "    return fact_list, current_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_split_point(point, chunk):\n",
    "    for i in range(point, len(chunk)):\n",
    "        if chunk[i] == ' ':\n",
    "            return i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_half(chunk):\n",
    "    midpoint = len(chunk) // 2\n",
    "\n",
    "    split_point = find_split_point(midpoint, chunk)\n",
    "    return [chunk[:split_point], chunk[split_point+1:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_third(chunk):\n",
    "    one_third = len(chunk) // 3\n",
    "    two_thirds = one_third * 2\n",
    "\n",
    "    first_split = find_split_point(one_third, chunk)\n",
    "    second_split = find_split_point(two_thirds, chunk)\n",
    "\n",
    "    return [chunk[:first_split], chunk[first_split+1:second_split], chunk[second_split+1:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_chunk(chunk, n):\n",
    "    if n == 2:\n",
    "        return split_half(chunk)\n",
    "    return split_third(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_summary_fact_rec1(page_content, chunk, user_language):\n",
    "    messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": prompting.summary_prompt_rec1(page_content, chunk, user_language)\n",
    "    },\n",
    "    ]\n",
    "    completion = api.get_completions_gpt35(messages, 0)\n",
    "    res_json = completion.choices[0].message.content\n",
    "\n",
    "    try:\n",
    "        res_json = ast.literal_eval(res_json)\n",
    "    except:\n",
    "        res_json = ast.literal_eval(res_json.replace(\"null\", \"None\"))\n",
    "    return res_json[\"context\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_summary_prompt(pdf_path, resource_id, user_language):\n",
    "    insert_query = \"\"\"\n",
    "        INSERT INTO fact (context, fact, resource_id, embeddings, summary, number)\n",
    "        VALUES (%s, %s, %s, %s, %s, %s);\n",
    "    \"\"\"\n",
    "    page_num = 1\n",
    "    reader = PdfReader(pdf_path)\n",
    "    for page in reader.pages:\n",
    "        p_content = page.extract_text()\n",
    "        fact_list, cur_summary = extract_summary_fact(p_content, user_language)\n",
    "        for f1 in fact_list:\n",
    "            token_count = prompt.count_tokens_tiktoken(f1)\n",
    "            if token_count > 600:\n",
    "                new_facts = split_chunk(f1, 3)\n",
    "                new_summary = extract_summary_fact_rec1(p_content, f1, user_language)\n",
    "                for f2 in new_facts:\n",
    "                    embed_result = api.get_embeddings_ada(f2)\n",
    "                    cursor.execute(insert_query, (new_summary, f2, resource_id, embed_result, cur_summary, page_num))\n",
    "                continue\n",
    "            elif token_count > 300:\n",
    "                new_facts = split_chunk(f1, 2)\n",
    "                new_summary = extract_summary_fact_rec1(p_content, f1, user_language)\n",
    "                for f2 in new_facts:\n",
    "                    embed_result = api.get_embeddings_ada(f2)\n",
    "                    cursor.execute(insert_query, (new_summary, f2, resource_id, embed_result, cur_summary, page_num))\n",
    "                continue\n",
    "            embed_result = api.get_embeddings_ada(f1)\n",
    "            cursor.execute(insert_query, (cur_summary, f1, resource_id, embed_result, cur_summary, page_num))\n",
    "        conn.commit()\n",
    "        page_num+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_path = \"\"\n",
    "resource_id = \"\"\n",
    "user_language = \"indonesian\"\n",
    "execute_fragment_prompt(pdf_path, resource_id, user_language)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Answer (Chat Completion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"question\"\n",
    "resource_ids = []\n",
    "user_lang = \"indonesia\"\n",
    "\n",
    "retrieved = db.get_retrieved_knowledge(cursor, question, resource_ids, 25)\n",
    "retrieved = [k[1] for k in retrieved]\n",
    "\n",
    "chat_prompt = prompt.get_chat_prompt(question, retrieved, memory_max_tokens=500, lang=user_lang)\n",
    "knowledges = prompt.get_knowledge_from_prompt(chat_prompt)\n",
    "chat_completion = api.get_completions_dolphin([\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": chat_prompt,\n",
    "    }\n",
    "], temp=0)\n",
    "\n",
    "response = chat_completion.choices[0].message.content"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
