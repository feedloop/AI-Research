{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import psycopg2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json, ast, time\n",
    "\n",
    "from importlib import reload\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from pgvector.psycopg2 import register_vector\n",
    "\n",
    "from utils import api, db, prompt, prompting\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'utils.prompting' from '/Volumes/Personal/Work/Feedloop/fact_ext/from_github/AI-Research/utils/prompting.py'>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reload(prompt)\n",
    "reload(api)\n",
    "reload(db)\n",
    "reload(prompting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_eval_prompt_token = 0\n",
    "total_eval_completion_token = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trial 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_answer = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_eval_prompt_token = 0\n",
    "total_eval_completion_token = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_path = \"./gpt_answer/SentWindow.json\"\n",
    "csv_path = \"./eval_result/SentWindow/Health_RAGAS_DIV_CAL1.csv\"\n",
    "doc_name = \"FL Healthcare Use Case - Medical Record\"\n",
    "user_language = \"Indonesian\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(json_path, 'r') as json_file:\n",
    "        json_answer_data = json.load(json_file)\n",
    "    \n",
    "cur_doc_answer = json_answer_data[doc_name]\n",
    "question_list, answer_list, context_list, truth_list = [], [], [], []\n",
    "for i in cur_doc_answer:\n",
    "    question_list.append(i['question'])\n",
    "    answer_list.append(i['answer'])\n",
    "    knowledge = \"\"\n",
    "    for k in i['contexts']:\n",
    "        knowledge+=f'{k}\\n'\n",
    "    context_list.append(knowledge)\n",
    "    \n",
    "    ground_truth = \"\"\n",
    "    for j in i['ground_truths']:\n",
    "        ground_truth+=f'{j}\\n'\n",
    "    truth_list.append(ground_truth)\n",
    "\n",
    "json_eval_result = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval_prompt = prompt.get_eval_ragas_cr_detail1(question_list[2], truth_list[2], answer_list[2], user_language)\n",
    "eval_prompt = prompt.get_eval_ragas_cr_trial1(question_list[2], answer_list[2])\n",
    "# print(eval_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_completion = api.get_completions_gpt4_turbo([\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": eval_prompt,\n",
    "    }\n",
    "], temp=0)\n",
    "total_eval_prompt_token+=eval_completion.usage.prompt_tokens\n",
    "total_eval_completion_token+=eval_completion.usage.completion_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = eval_completion.choices[0].message.content\n",
    "try:\n",
    "    response = ast.literal_eval(response)\n",
    "except:\n",
    "    response = ast.literal_eval(response.replace(\"null\", \"None\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print(response[\"relevancy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_answer[\"FL_SW_18\"] = response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./trial_eval/trialx.json', 'w') as json_file:\n",
    "    json.dump(response, json_file, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval prompt token: 30865\n",
      "eval completion token: 11042\n",
      "eval prompt token price: 0.31\n",
      "eval completion token price: 0.33\n"
     ]
    }
   ],
   "source": [
    "print(f\"eval prompt token: {total_eval_prompt_token}\\neval completion token: {total_eval_completion_token}\")\n",
    "eval_prompt_token_price = total_eval_prompt_token * 0.01 * 0.001\n",
    "eval_completion_token_price = total_eval_completion_token * 0.03 * 0.001\n",
    "print(f\"eval prompt token price: {round(eval_prompt_token_price,2)}\\neval completion token price: {round(eval_completion_token_price,2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trial1(json_path, doc_name):\n",
    "    global total_eval_prompt_token, total_eval_completion_token\n",
    "    with open(json_path, 'r') as json_file:\n",
    "        json_answer_data = json.load(json_file)\n",
    "    \n",
    "    cur_doc_answer = json_answer_data[doc_name]\n",
    "    dict_answer = {}\n",
    "    question_list, answer_list, context_list, truth_list = [], [], [], []\n",
    "    for i in cur_doc_answer:\n",
    "        question_list.append(i['question'])\n",
    "        answer_list.append(i['answer'])\n",
    "        knowledge = \"\"\n",
    "        for k in i['contexts']:\n",
    "            knowledge+=f'{k}\\n'\n",
    "        context_list.append(knowledge)\n",
    "        \n",
    "        ground_truth = \"\"\n",
    "        for j in i['ground_truths']:\n",
    "            ground_truth+=f'{j}\\n'\n",
    "        truth_list.append(ground_truth)\n",
    "    \n",
    "    for i in range(len(question_list)):\n",
    "        eval_prompt = prompt.get_eval_ragas_pr_detail3(question_list[i], answer_list[i], context_list[i])\n",
    "        eval_completion = api.get_completions_gpt4_turbo([\n",
    "            {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": eval_prompt,\n",
    "            }\n",
    "        ], temp=0)\n",
    "        total_eval_prompt_token+=eval_completion.usage.prompt_tokens\n",
    "        total_eval_completion_token+=eval_completion.usage.completion_tokens\n",
    "\n",
    "        response = eval_completion.choices[0].message.content\n",
    "        try:\n",
    "            response = ast.literal_eval(response)\n",
    "        except:\n",
    "            response = ast.literal_eval(response.replace(\"null\", \"None\"))\n",
    "        \n",
    "        dict_answer[\"Case\"+str(i)] = response\n",
    "    \n",
    "    with open('./trial_eval/trial1_no_example1.json', 'w') as json_file:\n",
    "        json.dump(dict_answer, json_file, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial1(json_path, doc_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./trial_eval/trial1_no_example1.json', 'r') as json_file:\n",
    "    dict_x = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data = []\n",
    "for case, values in dict_x.items():\n",
    "    total_attributed = sum(item['Attributed'] for item in values['classification'])\n",
    "    classification_length = len(values['classification'])\n",
    "    recall = total_attributed / classification_length\n",
    "    precision = values['verification']['verdict']\n",
    "    df_data.append({'Case': case, 'Recall': recall, 'Precision': precision})\n",
    "\n",
    "df = pd.DataFrame(df_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Case  Recall  Precision\n",
      "0    Case0     1.0          1\n",
      "1    Case1     1.0          1\n",
      "2    Case2     1.0          1\n",
      "3    Case3     1.0          1\n",
      "4    Case4     0.0          0\n",
      "5    Case5     1.0          1\n",
      "6    Case6     1.0          1\n",
      "7    Case7     1.0          1\n",
      "8    Case8     0.0          0\n",
      "9    Case9     1.0          1\n",
      "10  Case10     1.0          1\n",
      "11  Case11     1.0          1\n",
      "12  Case12     1.0          0\n",
      "13  Case13     1.0          1\n",
      "14  Case14     0.0          0\n",
      "15  Case15     1.0          1\n",
      "16  Case16     1.0          1\n",
      "17  Case17     1.0          1\n",
      "18  Case18     1.0          1\n",
      "19  Case19     1.0          1\n"
     ]
    }
   ],
   "source": [
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relevancy and correctness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trial2(json_path, doc_name, user_language):\n",
    "    global total_eval_prompt_token, total_eval_completion_token\n",
    "    with open(json_path, 'r') as json_file:\n",
    "        json_answer_data = json.load(json_file)\n",
    "    \n",
    "    cur_doc_answer = json_answer_data[doc_name]\n",
    "    dict_answer = {}\n",
    "    question_list, answer_list, context_list, truth_list = [], [], [], []\n",
    "    for i in cur_doc_answer:\n",
    "        question_list.append(i['question'])\n",
    "        answer_list.append(i['answer'])\n",
    "        knowledge = \"\"\n",
    "        for k in i['contexts']:\n",
    "            knowledge+=f'{k}\\n'\n",
    "        context_list.append(knowledge)\n",
    "        \n",
    "        ground_truth = \"\"\n",
    "        for j in i['ground_truths']:\n",
    "            ground_truth+=f'{j}\\n'\n",
    "        truth_list.append(ground_truth)\n",
    "\n",
    "    for i in range(len(question_list)):\n",
    "        eval_prompt = prompt.get_eval_ragas_cr_detail1(question_list[i], truth_list[i], answer_list[i])\n",
    "        eval_completion = api.get_completions_gpt4_turbo([\n",
    "            {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": eval_prompt,\n",
    "            }\n",
    "        ], temp=0)\n",
    "        total_eval_prompt_token+=eval_completion.usage.prompt_tokens\n",
    "        total_eval_completion_token+=eval_completion.usage.completion_tokens\n",
    "\n",
    "        response = eval_completion.choices[0].message.content\n",
    "        try:\n",
    "            response = ast.literal_eval(response)\n",
    "        except:\n",
    "            response = ast.literal_eval(response.replace(\"null\", \"None\"))\n",
    "        \n",
    "        # eval_prompt_r = prompt.get_eval_ragas_cr_trial1(question_list[i], answer_list[i])\n",
    "        # eval_completion_r = api.get_completions_gpt4_turbo([\n",
    "        #     {\n",
    "        #     \"role\": \"user\",\n",
    "        #     \"content\": eval_prompt_r,\n",
    "        #     }\n",
    "        # ], temp=0)\n",
    "        # total_eval_prompt_token+=eval_completion_r.usage.prompt_tokens\n",
    "        # total_eval_completion_token+=eval_completion_r.usage.completion_tokens\n",
    "\n",
    "        # response_r = eval_completion_r.choices[0].message.content\n",
    "        # try:\n",
    "        #     response_r = ast.literal_eval(response_r)\n",
    "        # except:\n",
    "        #     response_r = ast.literal_eval(response_r.replace(\"null\", \"None\"))\n",
    "        \n",
    "        response[\"question\"] = question_list[i]\n",
    "        # response[\"relevance\"] = response_r[\"relevancy\"]\n",
    "        dict_answer[\"Case\"+str(i)] = response\n",
    "    \n",
    "    with open('./trial_eval/trial_rc_no_example1d.json', 'w') as json_file:\n",
    "        json.dump(dict_answer, json_file, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(int(not bool(1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial2(json_path, doc_name, user_language)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./trial_eval/trial_rc_no_example1d.json', 'r') as json_file:\n",
    "    dict_cr = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_rel = [int(not bool(data[\"relevancy\"])) for case, data in dict_cr.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "correctness_values = {case: calculate_correctness(data['tp'], data['fp'], data['fn']) for case, data in dict_cr.items()}\n",
    "df['Correctness'] = df['Case'].map(correctness_values)\n",
    "sim_values = {case: calculate_similarity(data['question'], data['gen_question'], data['committal']) for case, data in dict_cr.items()}\n",
    "df['Relevancy'] = df['Case'].map(sim_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Case  Recall  Precision  Correctness  Relevancy\n",
      "0    Case0     1.0          1     1.000000      0.986\n",
      "1    Case1     1.0          1     1.000000      0.954\n",
      "2    Case2     1.0          1     0.000000      0.000\n",
      "3    Case3     1.0          1     0.857143      0.000\n",
      "4    Case4     0.0          0     0.000000      0.000\n",
      "5    Case5     1.0          1     0.666667      0.969\n",
      "6    Case6     1.0          1     0.000000      0.000\n",
      "7    Case7     1.0          1     0.000000      0.000\n",
      "8    Case8     0.0          0     0.000000      0.000\n",
      "9    Case9     1.0          1     1.000000      0.986\n",
      "10  Case10     1.0          1     0.750000      0.980\n",
      "11  Case11     1.0          1     0.000000      0.000\n",
      "12  Case12     1.0          0     0.000000      0.000\n",
      "13  Case13     1.0          1     0.000000      0.000\n",
      "14  Case14     0.0          0     0.000000      0.000\n",
      "15  Case15     1.0          1     1.000000      0.996\n",
      "16  Case16     1.0          1     0.000000      0.000\n",
      "17  Case17     1.0          1     0.000000      0.000\n",
      "18  Case18     1.0          1     0.000000      0.000\n",
      "19  Case19     1.0          1     0.666667      0.975\n"
     ]
    }
   ],
   "source": [
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "cr_resp = trial2(json_path, doc_name, user_language)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "cr_resp1 = trial2(json_path, doc_name, user_language)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"gen_question\": \"Kapan jadwal pertemuan Ahmad Surya dengan Dr. Indah Pratiwi selanjutnya untuk evaluasi pengobatan?\",\n",
      "    \"committal\": 1,\n",
      "    \"tp\": [\n",
      "        \"Pertemuan berikutnya antara Ahmad Surya dengan Dr. Indah Pratiwi untuk evaluasi pengobatan\",\n",
      "        \"Pertemuan dijadwalkan pada tanggal 5 Desember 2023\"\n",
      "    ],\n",
      "    \"fp\": [],\n",
      "    \"fn\": []\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(cr_resp.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"gen_question\": \"Kapan dijadwalkannya pertemuan berikutnya antara John Doe dengan Dr. Sarah Smith?\",\n",
      "    \"committal\": \"0\",\n",
      "    \"tp\": [],\n",
      "    \"fp\": [\"Maaf, saya tidak menemukan informasi tentang pertemuan berikutnya antara John Doe dengan Dr. Sarah Smith dalam dokumen yang disediakan.\"],\n",
      "    \"fn\": [\"Tanggal pertemuan berikutnya antara John Doe dengan Dr. Sarah Smith adalah 15 Agustus 2023.\"]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(cr_resp1.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_list(cur_doc_answer):\n",
    "    question_list, answer_list, context_list, truth_list = [], [], [], []\n",
    "    for i in cur_doc_answer:\n",
    "        question_list.append(i['question'])\n",
    "        answer_list.append(i['answer'])\n",
    "        knowledge = \"\"\n",
    "        for k in i['contexts']:\n",
    "            knowledge+=f'{k}\\n'\n",
    "        context_list.append(knowledge)\n",
    "        \n",
    "        ground_truth = \"\"\n",
    "        for j in i['ground_truths']:\n",
    "            ground_truth+=f'{j}\\n'\n",
    "        truth_list.append(ground_truth)\n",
    "    return question_list, answer_list, context_list, truth_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def increase_token_usage(completion):\n",
    "    global total_eval_prompt_token, total_eval_completion_token\n",
    "    total_eval_prompt_token+=completion.usage.prompt_tokens\n",
    "    total_eval_completion_token+=completion.usage.completion_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(eval_prompt):\n",
    "    return api.get_completions_gpt4_turbo([\n",
    "        {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": eval_prompt,\n",
    "        }\n",
    "    ], temp=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_json(response):\n",
    "    try:\n",
    "        res_json = ast.literal_eval(response)\n",
    "    except:\n",
    "        res_json = ast.literal_eval(response.replace(\"null\", \"None\"))\n",
    "    return res_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_eval_pr(question, answer, context):\n",
    "    eval_prompt = prompt.get_eval_ragas_pr_detail3(question, answer, context)\n",
    "    eval_completion = run_model(eval_prompt)\n",
    "    \n",
    "    increase_token_usage(eval_completion)\n",
    "\n",
    "    return convert_to_json(eval_completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_eval_cr(question, truth, answer):\n",
    "    eval_prompt = prompt.get_eval_ragas_cr_detail1(question, truth, answer)\n",
    "    eval_completion = run_model(eval_prompt)\n",
    "    \n",
    "    increase_token_usage(eval_completion)\n",
    "\n",
    "    return convert_to_json(eval_completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_correctness(tp, fp, fn):\n",
    "    tp_count = len(tp)\n",
    "    fp_count = len(fp)\n",
    "    fn_count = len(fn)\n",
    "    return round(tp_count / (tp_count + 0.5 * (fp_count + fn_count)), 2) if tp_count > 0 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_similarity(question, gen_question, noncommittal):\n",
    "    if isinstance(noncommittal, str):\n",
    "        noncommittal = int(noncommittal)\n",
    "    if noncommittal == 1:\n",
    "        return 0\n",
    "    embedding1_np = np.array(api.get_embeddings_ada(question))\n",
    "    embedding2_np = np.array(api.get_embeddings_ada(gen_question))\n",
    "    return round(cosine_similarity([embedding1_np], [embedding2_np])[0][0],3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_df(response):\n",
    "    df_data = []\n",
    "    for _, values in response.items():\n",
    "        total_attributed = sum(item['Attributed'] for item in values['classification'])\n",
    "        classification_length = len(values['classification'])\n",
    "        recall = total_attributed / classification_length\n",
    "        \n",
    "        precision = values['verification']['verdict']\n",
    "\n",
    "        correctness = calculate_correctness(values['tp'], values['fp'], values['fn'])\n",
    "    \n",
    "        similarity = calculate_similarity(values['question'], values['gen_question'], values['relevancy'])\n",
    "\n",
    "        df_data.append({'Context Recall': recall, 'Context Precision': precision, 'Answer Correctness': correctness, 'Answer Relevance': similarity})\n",
    "\n",
    "\n",
    "    return pd.DataFrame(df_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_eval(json_answer_data, doc_name, csv_path, json_output_path):\n",
    "    \n",
    "    # with open(json_path, 'r') as json_file:\n",
    "    #     json_answer_data = json.load(json_file)\n",
    "    \n",
    "    cur_doc_answer = json_answer_data[doc_name]\n",
    "    dict_res = {}\n",
    "    question_list, answer_list, context_list, truth_list = generate_list(cur_doc_answer)\n",
    "    \n",
    "    for i in range(len(question_list)):\n",
    "        response_pr = run_eval_pr(question_list[i], truth_list[i], context_list[i])\n",
    "        response_cr = run_eval_cr(question_list[i], truth_list[i], answer_list[i])\n",
    "        combined_res = {**response_pr, **response_cr}\n",
    "        combined_res[\"question\"] = question_list[i]\n",
    "        \n",
    "        dict_res[\"Case\"+str(i)] = combined_res\n",
    "        time.sleep(10)\n",
    "    \n",
    "    with open(json_output_path, 'w') as json_file:\n",
    "        json.dump(dict_res, json_file, indent=2)\n",
    "    # df = generate_df(dict_res)\n",
    "    # df.to_csv(csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_path = './gpt_answer/GPT35_ParentChild.json'\n",
    "with open(json_path, 'r') as json_file:\n",
    "    dict_gpt_answer = json.load(json_file)\n",
    "\n",
    "for key in dict_gpt_answer:\n",
    "    if key == \"Perbup Sleman Nomor 11.3 Tahun 2019 ttg Pengelolaan keuangan desa\":\n",
    "        continue\n",
    "    json_output_path = f\"./new_eval_res/GPT35_ParentChild/json/{key}.json\"\n",
    "    csv_path = f\"./new_eval_res/GPT35_ParentChild/csv/{key}.csv\"\n",
    "    doc_name = key\n",
    "    final_eval(dict_gpt_answer, doc_name, csv_path, json_output_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_csv(method, dict_gpt_answer):\n",
    "    for key in dict_gpt_answer:\n",
    "        if key == \"Perbup Sleman Nomor 11.3 Tahun 2019 ttg Pengelolaan keuangan desa\":\n",
    "            continue\n",
    "        with open(f'./new_eval_res/{method}/json/{key}.json', 'r') as json_file:\n",
    "            dict_temp = json.load(json_file)\n",
    "        df_temp = generate_df(dict_temp)\n",
    "        df_temp.to_csv(f'./new_eval_res/{method}/csv/{key}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_csv(\"GPT35_ParentChild\", dict_gpt_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval prompt token: 20258\n",
      "eval completion token: 6234\n",
      "eval prompt token price: 0.2\n",
      "eval completion token price: 0.19\n"
     ]
    }
   ],
   "source": [
    "print(f\"eval prompt token: {total_eval_prompt_token}\\neval completion token: {total_eval_completion_token}\")\n",
    "eval_prompt_token_price = total_eval_prompt_token * 0.01 * 0.001\n",
    "eval_completion_token_price = total_eval_completion_token * 0.03 * 0.001\n",
    "print(f\"eval prompt token price: {round(eval_prompt_token_price,2)}\\neval completion token price: {round(eval_completion_token_price,2)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
